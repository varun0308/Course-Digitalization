So now, now what is the problem with the few short prompting, right? We just discussed right now that few short prompting we might be able to achieve a lot of things. But when there is a multi step reasoning that is required, right? We just prompting it by saying, OK, so here is a question, here is the answer. Now I'll give you the question. You do the task. OK. That's what few short prompting kind of you know, let's believe it right now. This does not work when the reasoning that is required requires multiple steps, right? So now you're just saying, OK, give some explanation and then just saying answer is 11 between this question to this answer, there are so many steps and thoughts that are required. To generate this right. So now can we explicitly get the model to generate those thoughts and then answer the question rather than trying to jump to, you know, some number based on the context that is provided by the question, right. That's the key idea behind. This one chain of thought. Method right? So like this motivation is clear, right? Because when there are there is not enough information in the problem. There is enough information for reasoning and solving this problem that has to like you know, the multiple steps will go on in our head to come up with this answer. But it is not a language modelling task. You have to understand the difference. What is the LLM trying to do? It sees the series of tokens and trying to. The next token there is no logic that is implemented as part of that. It is like just influenced by the previous set of tokens. What tokens have higher probability and then just gets generated. But can we induce that logical step by step thinking as part of the, you know, prompting technique itself? To get LLM to use the strategy for a new problem. So the multi step reasoning tasks need thinking through intermediate steps, right? So now can we induce LLMS to do that? Yes, because LLMS that have been like, you know, 10s of billions of parameters and higher have the ability to, you know, learn from your. Examples to kind of mimic the process of thinking, not just generating the answers, right. So this was published in this paper, The chain of thought prompting. I think this is one of the most influential prompting techniques. Right, And almost everything else after this, we'll pretty much use this idea. OK, so let's look at the method. So goal is to endow language models with ability to generate chain of thought. OK, and implementation. Again, we are using the same few short prompting. You have to start looking at the subtle differences in the prompt and what we are trying to do. It's all there in the way the examples are given. It's not there in the structure if the information is there in the way examples are given. The way the demonstrations are done and the power of elements is to understand those example demonstrations and try to mimic the process when a new input is given. OK. Let me before we go to the example, let me show this, OK? So if you notice here the key. Difference. OK, this is your standard prompting, right? Where you're still giving, you know, one shot example here and then asking the question. The answer that is generated is wrong here. By the way, if you give the same thing today, it will answer correctly. OK, so this is when the public paper was published or whichever model that they used. For this, but let's focus on the idea, right? So now this blue piece of information that is added into your exemplar, which is. The one shot example that you're giving for the problem, this one thing is a prompt. So here if you see, Roger started with five balls, 2 cans of three tennis balls, each is 6 tennis balls. So 5 + 6 = 11. So we're kind of showing the steps. As part of the demonstration itself, instead of just trying to answer. So it's. More detailed, more specific information that we are illustrating that do this kind of a thing and answer the question then just trying to generate the answer. OK, So what LLM then does is given this question, look at the output now that green piece is the chain of thought it is generated. By learning from one example that was given here. OK. And since LLM spends time generating this information OK, and that information broken down like which may be implicitly available in the problem statement, all that gets more explicitly made available as part of the context. Purely by using verbal reasoning, the answer that is generated can be. Much better and much more accurate than trying to do that. From the original, original, you know, problem and directly trying to guess that. OK, so it's almost like don't guess the answer, but follow some steps to break down the information and the chain of thought. How will you go about doing this and then generate those steps as well as the answer, OK, this helps the LLM to focus on the right kind of information. And the right. In the problem and then generate the answer. So the context that it gets is much much better right for answering the question than the original question is. Is this clear? So not the like the the detailed prompts. I mean it's not like they're just using one example. Like I think more examples are used as part of the prompting for the sake of brevity in presentation. They're just showing one example. So if you look at the paper, you can see more detailed prompts in the end, OK, but the idea remains just the same, right? So. We are inducing a particular style of thinking. By giving examples of that style of thinking. OK, so as such it is just a few short example, but the way we are structuring the examples and demonstrations is where we have control. It's almost like we are using. Solved problems. Kind of a context we are giving to the model and kind of encouraging it to use similar kind of thinking to solve any new problem given. OK. So there are, you know, more examples here, right? How the model kind of breaks down? For different types of problems, how the question information and the question is broken down into your chain of thought and answer is provided. So you might be finally interested only in the answer, right? So you have the option of inserting OK, you don't have to follow the exact same pattern. Right. You could, you know, model this thing one second, I'll just create one place where. OK. So you can say for in your demonstrations.  Oh, so you know that part. So we'll have like you can say question. No, this is bye. OK, you can use this question and give something you can say reasoning. And then you can provide something and then you can say answer. Is this right? You could break that down explicitly also, which is what will be done by the later prompts, but the idea is still the same, right? So. But this will help you separate the reasoning and answer maybe for your parsing stages later. Or you want to take the reasoning and maybe you want to use that reasoning and use the external tool to solve the problem and fill it in here. So this kind of this kind of adding these additional scaffoldings can give you a little more control. But the idea is the way we want the model to think, we can induce that thinking by just giving examples through future. Prompting, right? So what are some important advantages here? You can help us decompose the problem complex problem into intermediate steps, allowing more information processing, you know, for the LLM to generate reliable responses. So interpretability if you want to use the reasoning that is generated to give it to the end user or use it in some other way. In order to understand the model behaviour of why this answer was suggested, you could use the same technique. OK, so useful for, you know, math problems or reasoning where multi step reasonings is involved, right. So it's not just I can I can't read, I can't read the answer from the question directly. I need to do a few steps before answering there you can use chain of thought prompting. OK, and it's easy to elicit in LL Mississippi. This kind of a behaviour using few short part. So any questions about? Chain of thought because this is going to be a very crucial technique that you'll have to keep in mind. Many of them will build on top of this.