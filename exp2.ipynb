{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from utils import save_audio_from_mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base.en\")      # turbo is recommended in sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio.mp3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_audio_from_mp4(\"Ses 03.mp4\", audio_file=\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:37<00:00, 3.90MiB/s]\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" So, now what is the problem with few short prompting right we just discussed right now that few short prompting we might be able to achieve lot of things, but when there is a multi-step reasoning that is required right we just prompting it by saying ok. So, here is a question, here is also now I will give you the question you do the task ok. That is what few short prompting kind of you know let us believe right. Now, this does not work when the reasoning that is required requires multiple steps right. So, now you are just saying ok, you give some explanation and then just saying answer is an event between this question to this answer there are so many steps and thoughts that are required to generate this right. So, now can we explicitly get the model to generate those thoughts and then answer the question rather than trying to jump to you know some number based on the context that is provided by the question right. That is the key idea behind this one chain of thought like right. So, like this motivation is clear right because when there are there is not enough information in the problem there is enough information for reasoning and solving this problem that has to like you know the multiple steps will we go on in our head to come up with this answer. But it is not a language modeling task you have to understand the difference. What is the LLM trying to do? It sees the series of tokens and trying to predict the next token there is no logic that is implemented as part of that. It is like just influenced by the previous set of tokens what tokens have higher probability and then just gets stimulated. But can we induce that logical step by step thinking as part of the prompting technique itself to get LLM to use this strategy for a new problem. So, the multi-step reasoning tasks need thinking through intermediate steps right. So, now can we induce LLM to do that? Yes, because LLM that have been like you know tens of billions of parameters and higher have the ability to you know learn from your examples to kind of mimic the process of thinking not just generating the answers right. So, this was published in this paper the chain of thought prompting I think this is one of the most influential prompting techniques right and almost everything else after this will pretty much use this idea. So, let us look at the method. So, goal is to endow language models with ability to generate chain of thought and implementation again we are using the same few short prompting you will have to start looking at the subtle differences in the prompt and what we are trying to do it is all there in the way the examples are given it is not there in the structure if the information is there in the way examples are given way the demonstrations are done. And the power of LLM is to understand those example demonstrations and try to mimic the process when new input is given ok. So, let me before go to the example let me show this itself. So, if you notice here the key difference this is your standard prompting right where you are still giving you know one short example here and then asking the question the answer that is generated is wrong there. By the way if you give the same thing today it will answer the title ok. So, this is when the paper was published or whichever model that they used for this but let us focus on the idea right. So, now this blue piece of information that is added into your example which is the one short example that you are giving for the prompt this one thing is the prompt right right. So, here if you see Roger started with 5 balls two cans of three tennis balls each these six tennis balls so 5 plus 6 equals 11. So, we are kind of showing the steps as part of the demonstration itself instead of just trying to ask. So, it is a more detailed more specific information that we are illustrating that do this kind of a thing and answer the question then just trying to generate the answer ok. So, what LLM then does is given this question look at the output now that green piece is the chain of thought it is generated by learning from one example that is given here ok. And here since LLM spends kind generating this information ok and that information broken down like which may be implicitly available in the problem statement all that gets more explicitly made available as part of a context. Purely by using verbal reasoning the answer that is generated can be much better and much more accurate than trying to do that from the original original problem and directly trying to guess the answer ok. So, it is almost like do not guess the answer but follow some steps to break down the information and the chain of thought how will you go about doing this and then generate those steps as well as the answer ok. This helps the LLM to focus on the right kind of information and the right in the problem and then generate the answer. So, the context that it gets is much much better for answering the question than the original question is this is clear. So, not like the the detailed prompts I mean it is not like they are just using one example like I think more examples are used as part of the prompting for the sake of brevity in presentation they are just showing one example. So, if you look at the paper you can see more detailed prompts in the end. But the idea remains just the same right. So, we are inducing a particular style of thinking by giving examples of that style of thinking. So, as such it is just a few short example but the way we are structuring the examples and demonstrations is where we have control. It is almost like we are using a solved problems kind of the context we are giving to the model and kind of encouraging it to use similar kind of thinking to solve any new problem given. So, there are you know more examples here how the model you kind of breaks down for different types of problems how the information the question is broken down into your chain of thought and the answer is provided. So, you might be finally interested only in the answer right. So, you have the option of inserting okay you do not have to follow the exact same pattern right. You could you know model this thing one second object like create one place where okay. So, you can say for any your demonstrations with okay. So, you go that part. So, we will have like you can say question no this is back okay. You can use this question and give something you can say reasoning and then you can provide something and then you can say answer is this right. You could break that down explicitly also which is what will be done by later to the props. But the idea is still the same right. So, but this will help you separate the reasoning and answer maybe for your parsing stages later or you want to take the reasoning and maybe you want to use that reasoning and use an external tool to solve the problem and fill it in here. So, this kind of this kind of adding these additional scaffolding's can give you little more control. But the idea is the way we want the model to think we can induce that thinking by just giving examples through few short prompting right. So, what are some important advantages here? You can help us decompose the problem complex problem into intermediate steps allowing more information processing you know for the LLM to generate the reliable responses. So, interpretability if you want to use the reasoning that is generated to give it to the end user or use it in some other way in order to understand the model behavior on why this answer was suggested you could use this end user. So, useful for you know math problems or reasoning where multi-step reasoning is involved so, it is not just I can I cannot read I cannot read the answer from the question directly. I need to do few steps before answering there you can use chain of thought prompting. And it is easy to elicit in LLM's this kind of a behavior using few short point. So, any questions about chain of thought because this is going to be a very crucial technique that you will have to keep in mind many of them will build on the focus.\",\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 22.12,\n",
       "   'text': ' So, now what is the problem with few short prompting right we just discussed right now that',\n",
       "   'tokens': [50364,\n",
       "    407,\n",
       "    11,\n",
       "    586,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    1154,\n",
       "    365,\n",
       "    1326,\n",
       "    2099,\n",
       "    12391,\n",
       "    278,\n",
       "    558,\n",
       "    321,\n",
       "    445,\n",
       "    7152,\n",
       "    558,\n",
       "    586,\n",
       "    300,\n",
       "    51470],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.29612313940169965,\n",
       "   'compression_ratio': 1.4919354838709677,\n",
       "   'no_speech_prob': 0.013691898435354233},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 22.12,\n",
       "   'end': 28.04,\n",
       "   'text': ' few short prompting we might be able to achieve lot of things, but when there is a multi-step',\n",
       "   'tokens': [51470,\n",
       "    1326,\n",
       "    2099,\n",
       "    12391,\n",
       "    278,\n",
       "    321,\n",
       "    1062,\n",
       "    312,\n",
       "    1075,\n",
       "    281,\n",
       "    4584,\n",
       "    688,\n",
       "    295,\n",
       "    721,\n",
       "    11,\n",
       "    457,\n",
       "    562,\n",
       "    456,\n",
       "    307,\n",
       "    257,\n",
       "    4825,\n",
       "    12,\n",
       "    16792,\n",
       "    51766],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.29612313940169965,\n",
       "   'compression_ratio': 1.4919354838709677,\n",
       "   'no_speech_prob': 0.013691898435354233},\n",
       "  {'id': 2,\n",
       "   'seek': 2804,\n",
       "   'start': 28.04,\n",
       "   'end': 37.08,\n",
       "   'text': ' reasoning that is required right we just prompting it by saying ok. So, here is a question, here is',\n",
       "   'tokens': [50364,\n",
       "    21577,\n",
       "    300,\n",
       "    307,\n",
       "    4739,\n",
       "    558,\n",
       "    321,\n",
       "    445,\n",
       "    12391,\n",
       "    278,\n",
       "    309,\n",
       "    538,\n",
       "    1566,\n",
       "    3133,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    510,\n",
       "    307,\n",
       "    257,\n",
       "    1168,\n",
       "    11,\n",
       "    510,\n",
       "    307,\n",
       "    50816],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17723760212937448,\n",
       "   'compression_ratio': 1.7,\n",
       "   'no_speech_prob': 0.007384474854916334},\n",
       "  {'id': 3,\n",
       "   'seek': 2804,\n",
       "   'start': 37.08,\n",
       "   'end': 46.120000000000005,\n",
       "   'text': ' also now I will give you the question you do the task ok. That is what few short prompting kind',\n",
       "   'tokens': [50816,\n",
       "    611,\n",
       "    586,\n",
       "    286,\n",
       "    486,\n",
       "    976,\n",
       "    291,\n",
       "    264,\n",
       "    1168,\n",
       "    291,\n",
       "    360,\n",
       "    264,\n",
       "    5633,\n",
       "    3133,\n",
       "    13,\n",
       "    663,\n",
       "    307,\n",
       "    437,\n",
       "    1326,\n",
       "    2099,\n",
       "    12391,\n",
       "    278,\n",
       "    733,\n",
       "    51268],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17723760212937448,\n",
       "   'compression_ratio': 1.7,\n",
       "   'no_speech_prob': 0.007384474854916334},\n",
       "  {'id': 4,\n",
       "   'seek': 2804,\n",
       "   'start': 46.120000000000005,\n",
       "   'end': 54.76,\n",
       "   'text': ' of you know let us believe right. Now, this does not work when the reasoning that is required',\n",
       "   'tokens': [51268,\n",
       "    295,\n",
       "    291,\n",
       "    458,\n",
       "    718,\n",
       "    505,\n",
       "    1697,\n",
       "    558,\n",
       "    13,\n",
       "    823,\n",
       "    11,\n",
       "    341,\n",
       "    775,\n",
       "    406,\n",
       "    589,\n",
       "    562,\n",
       "    264,\n",
       "    21577,\n",
       "    300,\n",
       "    307,\n",
       "    4739,\n",
       "    51700],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17723760212937448,\n",
       "   'compression_ratio': 1.7,\n",
       "   'no_speech_prob': 0.007384474854916334},\n",
       "  {'id': 5,\n",
       "   'seek': 5476,\n",
       "   'start': 55.72,\n",
       "   'end': 61.64,\n",
       "   'text': ' requires multiple steps right. So, now you are just saying ok, you give some explanation and',\n",
       "   'tokens': [50412,\n",
       "    7029,\n",
       "    3866,\n",
       "    4439,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    586,\n",
       "    291,\n",
       "    366,\n",
       "    445,\n",
       "    1566,\n",
       "    3133,\n",
       "    11,\n",
       "    291,\n",
       "    976,\n",
       "    512,\n",
       "    10835,\n",
       "    293,\n",
       "    50708],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16706769168376923,\n",
       "   'compression_ratio': 1.6608187134502923,\n",
       "   'no_speech_prob': 0.007960626855492592},\n",
       "  {'id': 6,\n",
       "   'seek': 5476,\n",
       "   'start': 61.64,\n",
       "   'end': 69.72,\n",
       "   'text': ' then just saying answer is an event between this question to this answer there are so many steps',\n",
       "   'tokens': [50708,\n",
       "    550,\n",
       "    445,\n",
       "    1566,\n",
       "    1867,\n",
       "    307,\n",
       "    364,\n",
       "    2280,\n",
       "    1296,\n",
       "    341,\n",
       "    1168,\n",
       "    281,\n",
       "    341,\n",
       "    1867,\n",
       "    456,\n",
       "    366,\n",
       "    370,\n",
       "    867,\n",
       "    4439,\n",
       "    51112],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16706769168376923,\n",
       "   'compression_ratio': 1.6608187134502923,\n",
       "   'no_speech_prob': 0.007960626855492592},\n",
       "  {'id': 7,\n",
       "   'seek': 5476,\n",
       "   'start': 69.72,\n",
       "   'end': 79.32,\n",
       "   'text': ' and thoughts that are required to generate this right. So, now can we explicitly get the model',\n",
       "   'tokens': [51112,\n",
       "    293,\n",
       "    4598,\n",
       "    300,\n",
       "    366,\n",
       "    4739,\n",
       "    281,\n",
       "    8460,\n",
       "    341,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    586,\n",
       "    393,\n",
       "    321,\n",
       "    20803,\n",
       "    483,\n",
       "    264,\n",
       "    2316,\n",
       "    51592],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16706769168376923,\n",
       "   'compression_ratio': 1.6608187134502923,\n",
       "   'no_speech_prob': 0.007960626855492592},\n",
       "  {'id': 8,\n",
       "   'seek': 7932,\n",
       "   'start': 80.03999999999999,\n",
       "   'end': 88.6,\n",
       "   'text': ' to generate those thoughts and then answer the question rather than trying to jump to you know',\n",
       "   'tokens': [50400,\n",
       "    281,\n",
       "    8460,\n",
       "    729,\n",
       "    4598,\n",
       "    293,\n",
       "    550,\n",
       "    1867,\n",
       "    264,\n",
       "    1168,\n",
       "    2831,\n",
       "    813,\n",
       "    1382,\n",
       "    281,\n",
       "    3012,\n",
       "    281,\n",
       "    291,\n",
       "    458,\n",
       "    50828],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11287335462348405,\n",
       "   'compression_ratio': 1.5,\n",
       "   'no_speech_prob': 0.006620889063924551},\n",
       "  {'id': 9,\n",
       "   'seek': 7932,\n",
       "   'start': 88.6,\n",
       "   'end': 97.08,\n",
       "   'text': ' some number based on the context that is provided by the question right. That is the key idea behind',\n",
       "   'tokens': [50828,\n",
       "    512,\n",
       "    1230,\n",
       "    2361,\n",
       "    322,\n",
       "    264,\n",
       "    4319,\n",
       "    300,\n",
       "    307,\n",
       "    5649,\n",
       "    538,\n",
       "    264,\n",
       "    1168,\n",
       "    558,\n",
       "    13,\n",
       "    663,\n",
       "    307,\n",
       "    264,\n",
       "    2141,\n",
       "    1558,\n",
       "    2261,\n",
       "    51252],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11287335462348405,\n",
       "   'compression_ratio': 1.5,\n",
       "   'no_speech_prob': 0.006620889063924551},\n",
       "  {'id': 10,\n",
       "   'seek': 9708,\n",
       "   'start': 97.96,\n",
       "   'end': 111.88,\n",
       "   'text': ' this one chain of thought like right. So, like this motivation is clear right because when',\n",
       "   'tokens': [50408,\n",
       "    341,\n",
       "    472,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    411,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    411,\n",
       "    341,\n",
       "    12335,\n",
       "    307,\n",
       "    1850,\n",
       "    558,\n",
       "    570,\n",
       "    562,\n",
       "    51104],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.22132643631526402,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0035137042868882418},\n",
       "  {'id': 11,\n",
       "   'seek': 9708,\n",
       "   'start': 112.6,\n",
       "   'end': 118.2,\n",
       "   'text': ' there are there is not enough information in the problem there is enough information for',\n",
       "   'tokens': [51140,\n",
       "    456,\n",
       "    366,\n",
       "    456,\n",
       "    307,\n",
       "    406,\n",
       "    1547,\n",
       "    1589,\n",
       "    294,\n",
       "    264,\n",
       "    1154,\n",
       "    456,\n",
       "    307,\n",
       "    1547,\n",
       "    1589,\n",
       "    337,\n",
       "    51420],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.22132643631526402,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0035137042868882418},\n",
       "  {'id': 12,\n",
       "   'seek': 9708,\n",
       "   'start': 118.75999999999999,\n",
       "   'end': 123.96,\n",
       "   'text': ' reasoning and solving this problem that has to like you know the multiple steps will',\n",
       "   'tokens': [51448,\n",
       "    21577,\n",
       "    293,\n",
       "    12606,\n",
       "    341,\n",
       "    1154,\n",
       "    300,\n",
       "    575,\n",
       "    281,\n",
       "    411,\n",
       "    291,\n",
       "    458,\n",
       "    264,\n",
       "    3866,\n",
       "    4439,\n",
       "    486,\n",
       "    51708],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.22132643631526402,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0035137042868882418},\n",
       "  {'id': 13,\n",
       "   'seek': 12396,\n",
       "   'start': 124.03999999999999,\n",
       "   'end': 132.12,\n",
       "   'text': ' we go on in our head to come up with this answer. But it is not a language modeling task you',\n",
       "   'tokens': [50368,\n",
       "    321,\n",
       "    352,\n",
       "    322,\n",
       "    294,\n",
       "    527,\n",
       "    1378,\n",
       "    281,\n",
       "    808,\n",
       "    493,\n",
       "    365,\n",
       "    341,\n",
       "    1867,\n",
       "    13,\n",
       "    583,\n",
       "    309,\n",
       "    307,\n",
       "    406,\n",
       "    257,\n",
       "    2856,\n",
       "    15983,\n",
       "    5633,\n",
       "    291,\n",
       "    50772],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16504990652705845,\n",
       "   'compression_ratio': 1.6271929824561404,\n",
       "   'no_speech_prob': 0.003919258248060942},\n",
       "  {'id': 14,\n",
       "   'seek': 12396,\n",
       "   'start': 132.12,\n",
       "   'end': 137.95999999999998,\n",
       "   'text': ' have to understand the difference. What is the LLM trying to do? It sees the series of tokens and',\n",
       "   'tokens': [50772,\n",
       "    362,\n",
       "    281,\n",
       "    1223,\n",
       "    264,\n",
       "    2649,\n",
       "    13,\n",
       "    708,\n",
       "    307,\n",
       "    264,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    1382,\n",
       "    281,\n",
       "    360,\n",
       "    30,\n",
       "    467,\n",
       "    8194,\n",
       "    264,\n",
       "    2638,\n",
       "    295,\n",
       "    22667,\n",
       "    293,\n",
       "    51064],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16504990652705845,\n",
       "   'compression_ratio': 1.6271929824561404,\n",
       "   'no_speech_prob': 0.003919258248060942},\n",
       "  {'id': 15,\n",
       "   'seek': 12396,\n",
       "   'start': 137.95999999999998,\n",
       "   'end': 142.44,\n",
       "   'text': ' trying to predict the next token there is no logic that is implemented as part of that.',\n",
       "   'tokens': [51064,\n",
       "    1382,\n",
       "    281,\n",
       "    6069,\n",
       "    264,\n",
       "    958,\n",
       "    14862,\n",
       "    456,\n",
       "    307,\n",
       "    572,\n",
       "    9952,\n",
       "    300,\n",
       "    307,\n",
       "    12270,\n",
       "    382,\n",
       "    644,\n",
       "    295,\n",
       "    300,\n",
       "    13,\n",
       "    51288],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16504990652705845,\n",
       "   'compression_ratio': 1.6271929824561404,\n",
       "   'no_speech_prob': 0.003919258248060942},\n",
       "  {'id': 16,\n",
       "   'seek': 12396,\n",
       "   'start': 142.44,\n",
       "   'end': 147.95999999999998,\n",
       "   'text': ' It is like just influenced by the previous set of tokens what tokens have higher probability',\n",
       "   'tokens': [51288,\n",
       "    467,\n",
       "    307,\n",
       "    411,\n",
       "    445,\n",
       "    15269,\n",
       "    538,\n",
       "    264,\n",
       "    3894,\n",
       "    992,\n",
       "    295,\n",
       "    22667,\n",
       "    437,\n",
       "    22667,\n",
       "    362,\n",
       "    2946,\n",
       "    8482,\n",
       "    51564],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16504990652705845,\n",
       "   'compression_ratio': 1.6271929824561404,\n",
       "   'no_speech_prob': 0.003919258248060942},\n",
       "  {'id': 17,\n",
       "   'seek': 14796,\n",
       "   'start': 148.04000000000002,\n",
       "   'end': 157.24,\n",
       "   'text': ' and then just gets stimulated. But can we induce that logical step by step thinking as part of the',\n",
       "   'tokens': [50368,\n",
       "    293,\n",
       "    550,\n",
       "    445,\n",
       "    2170,\n",
       "    8983,\n",
       "    6987,\n",
       "    13,\n",
       "    583,\n",
       "    393,\n",
       "    321,\n",
       "    41263,\n",
       "    300,\n",
       "    14978,\n",
       "    1823,\n",
       "    538,\n",
       "    1823,\n",
       "    1953,\n",
       "    382,\n",
       "    644,\n",
       "    295,\n",
       "    264,\n",
       "    50828],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2596574995252821,\n",
       "   'compression_ratio': 1.3435114503816794,\n",
       "   'no_speech_prob': 0.0027663493528962135},\n",
       "  {'id': 18,\n",
       "   'seek': 14796,\n",
       "   'start': 158.28,\n",
       "   'end': 164.92000000000002,\n",
       "   'text': ' prompting technique itself to get LLM to use this strategy for a new problem.',\n",
       "   'tokens': [50880,\n",
       "    12391,\n",
       "    278,\n",
       "    6532,\n",
       "    2564,\n",
       "    281,\n",
       "    483,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    281,\n",
       "    764,\n",
       "    341,\n",
       "    5206,\n",
       "    337,\n",
       "    257,\n",
       "    777,\n",
       "    1154,\n",
       "    13,\n",
       "    51212],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2596574995252821,\n",
       "   'compression_ratio': 1.3435114503816794,\n",
       "   'no_speech_prob': 0.0027663493528962135},\n",
       "  {'id': 19,\n",
       "   'seek': 16492,\n",
       "   'start': 165.48,\n",
       "   'end': 179.64,\n",
       "   'text': ' So, the multi-step reasoning tasks need thinking through intermediate steps right. So, now',\n",
       "   'tokens': [50392,\n",
       "    407,\n",
       "    11,\n",
       "    264,\n",
       "    4825,\n",
       "    12,\n",
       "    16792,\n",
       "    21577,\n",
       "    9608,\n",
       "    643,\n",
       "    1953,\n",
       "    807,\n",
       "    19376,\n",
       "    4439,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    586,\n",
       "    51100],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18516290664672852,\n",
       "   'compression_ratio': 1.3381294964028776,\n",
       "   'no_speech_prob': 0.022192971780896187},\n",
       "  {'id': 20,\n",
       "   'seek': 16492,\n",
       "   'start': 181.0,\n",
       "   'end': 188.44,\n",
       "   'text': ' can we induce LLM to do that? Yes, because LLM that have been like you know tens of billions of',\n",
       "   'tokens': [51168,\n",
       "    393,\n",
       "    321,\n",
       "    41263,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    281,\n",
       "    360,\n",
       "    300,\n",
       "    30,\n",
       "    1079,\n",
       "    11,\n",
       "    570,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    300,\n",
       "    362,\n",
       "    668,\n",
       "    411,\n",
       "    291,\n",
       "    458,\n",
       "    10688,\n",
       "    295,\n",
       "    17375,\n",
       "    295,\n",
       "    51540],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18516290664672852,\n",
       "   'compression_ratio': 1.3381294964028776,\n",
       "   'no_speech_prob': 0.022192971780896187},\n",
       "  {'id': 21,\n",
       "   'seek': 18844,\n",
       "   'start': 188.44,\n",
       "   'end': 200.35999999999999,\n",
       "   'text': ' parameters and higher have the ability to you know learn from your examples to kind of mimic',\n",
       "   'tokens': [50364,\n",
       "    9834,\n",
       "    293,\n",
       "    2946,\n",
       "    362,\n",
       "    264,\n",
       "    3485,\n",
       "    281,\n",
       "    291,\n",
       "    458,\n",
       "    1466,\n",
       "    490,\n",
       "    428,\n",
       "    5110,\n",
       "    281,\n",
       "    733,\n",
       "    295,\n",
       "    31075,\n",
       "    50960],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11353541189624418,\n",
       "   'compression_ratio': 1.5737704918032787,\n",
       "   'no_speech_prob': 0.0013158521614968777},\n",
       "  {'id': 22,\n",
       "   'seek': 18844,\n",
       "   'start': 200.35999999999999,\n",
       "   'end': 208.52,\n",
       "   'text': ' the process of thinking not just generating the answers right. So, this was published in this',\n",
       "   'tokens': [50960,\n",
       "    264,\n",
       "    1399,\n",
       "    295,\n",
       "    1953,\n",
       "    406,\n",
       "    445,\n",
       "    17746,\n",
       "    264,\n",
       "    6338,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    341,\n",
       "    390,\n",
       "    6572,\n",
       "    294,\n",
       "    341,\n",
       "    51368],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11353541189624418,\n",
       "   'compression_ratio': 1.5737704918032787,\n",
       "   'no_speech_prob': 0.0013158521614968777},\n",
       "  {'id': 23,\n",
       "   'seek': 18844,\n",
       "   'start': 208.52,\n",
       "   'end': 213.56,\n",
       "   'text': ' paper the chain of thought prompting I think this is one of the most influential prompting techniques',\n",
       "   'tokens': [51368,\n",
       "    3035,\n",
       "    264,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    12391,\n",
       "    278,\n",
       "    286,\n",
       "    519,\n",
       "    341,\n",
       "    307,\n",
       "    472,\n",
       "    295,\n",
       "    264,\n",
       "    881,\n",
       "    22215,\n",
       "    12391,\n",
       "    278,\n",
       "    7512,\n",
       "    51620],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11353541189624418,\n",
       "   'compression_ratio': 1.5737704918032787,\n",
       "   'no_speech_prob': 0.0013158521614968777},\n",
       "  {'id': 24,\n",
       "   'seek': 21356,\n",
       "   'start': 214.52,\n",
       "   'end': 220.04,\n",
       "   'text': ' right and almost everything else after this will pretty much use this idea.',\n",
       "   'tokens': [50412,\n",
       "    558,\n",
       "    293,\n",
       "    1920,\n",
       "    1203,\n",
       "    1646,\n",
       "    934,\n",
       "    341,\n",
       "    486,\n",
       "    1238,\n",
       "    709,\n",
       "    764,\n",
       "    341,\n",
       "    1558,\n",
       "    13,\n",
       "    50688],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.191346863169729,\n",
       "   'compression_ratio': 1.6603773584905661,\n",
       "   'no_speech_prob': 0.0064932554960250854},\n",
       "  {'id': 25,\n",
       "   'seek': 21356,\n",
       "   'start': 223.32,\n",
       "   'end': 229.0,\n",
       "   'text': ' So, let us look at the method. So, goal is to endow language models with ability to generate',\n",
       "   'tokens': [50852,\n",
       "    407,\n",
       "    11,\n",
       "    718,\n",
       "    505,\n",
       "    574,\n",
       "    412,\n",
       "    264,\n",
       "    3170,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    3387,\n",
       "    307,\n",
       "    281,\n",
       "    917,\n",
       "    305,\n",
       "    2856,\n",
       "    5245,\n",
       "    365,\n",
       "    3485,\n",
       "    281,\n",
       "    8460,\n",
       "    51136],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.191346863169729,\n",
       "   'compression_ratio': 1.6603773584905661,\n",
       "   'no_speech_prob': 0.0064932554960250854},\n",
       "  {'id': 26,\n",
       "   'seek': 21356,\n",
       "   'start': 229.0,\n",
       "   'end': 235.4,\n",
       "   'text': ' chain of thought and implementation again we are using the same few short prompting',\n",
       "   'tokens': [51136,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    293,\n",
       "    11420,\n",
       "    797,\n",
       "    321,\n",
       "    366,\n",
       "    1228,\n",
       "    264,\n",
       "    912,\n",
       "    1326,\n",
       "    2099,\n",
       "    12391,\n",
       "    278,\n",
       "    51456],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.191346863169729,\n",
       "   'compression_ratio': 1.6603773584905661,\n",
       "   'no_speech_prob': 0.0064932554960250854},\n",
       "  {'id': 27,\n",
       "   'seek': 21356,\n",
       "   'start': 235.4,\n",
       "   'end': 241.72,\n",
       "   'text': ' you will have to start looking at the subtle differences in the prompt and what we are trying to do',\n",
       "   'tokens': [51456,\n",
       "    291,\n",
       "    486,\n",
       "    362,\n",
       "    281,\n",
       "    722,\n",
       "    1237,\n",
       "    412,\n",
       "    264,\n",
       "    13743,\n",
       "    7300,\n",
       "    294,\n",
       "    264,\n",
       "    12391,\n",
       "    293,\n",
       "    437,\n",
       "    321,\n",
       "    366,\n",
       "    1382,\n",
       "    281,\n",
       "    360,\n",
       "    51772],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.191346863169729,\n",
       "   'compression_ratio': 1.6603773584905661,\n",
       "   'no_speech_prob': 0.0064932554960250854},\n",
       "  {'id': 28,\n",
       "   'seek': 24172,\n",
       "   'start': 242.44,\n",
       "   'end': 248.36,\n",
       "   'text': ' it is all there in the way the examples are given it is not there in the structure if the',\n",
       "   'tokens': [50400,\n",
       "    309,\n",
       "    307,\n",
       "    439,\n",
       "    456,\n",
       "    294,\n",
       "    264,\n",
       "    636,\n",
       "    264,\n",
       "    5110,\n",
       "    366,\n",
       "    2212,\n",
       "    309,\n",
       "    307,\n",
       "    406,\n",
       "    456,\n",
       "    294,\n",
       "    264,\n",
       "    3877,\n",
       "    498,\n",
       "    264,\n",
       "    50696],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1263718605041504,\n",
       "   'compression_ratio': 1.9214285714285715,\n",
       "   'no_speech_prob': 0.0037031839601695538},\n",
       "  {'id': 29,\n",
       "   'seek': 24172,\n",
       "   'start': 248.36,\n",
       "   'end': 253.56,\n",
       "   'text': ' information is there in the way examples are given way the demonstrations are done.',\n",
       "   'tokens': [50696,\n",
       "    1589,\n",
       "    307,\n",
       "    456,\n",
       "    294,\n",
       "    264,\n",
       "    636,\n",
       "    5110,\n",
       "    366,\n",
       "    2212,\n",
       "    636,\n",
       "    264,\n",
       "    34714,\n",
       "    366,\n",
       "    1096,\n",
       "    13,\n",
       "    50956],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1263718605041504,\n",
       "   'compression_ratio': 1.9214285714285715,\n",
       "   'no_speech_prob': 0.0037031839601695538},\n",
       "  {'id': 30,\n",
       "   'seek': 24172,\n",
       "   'start': 254.52,\n",
       "   'end': 261.64,\n",
       "   'text': ' And the power of LLM is to understand those example demonstrations and try to mimic the process',\n",
       "   'tokens': [51004,\n",
       "    400,\n",
       "    264,\n",
       "    1347,\n",
       "    295,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    307,\n",
       "    281,\n",
       "    1223,\n",
       "    729,\n",
       "    1365,\n",
       "    34714,\n",
       "    293,\n",
       "    853,\n",
       "    281,\n",
       "    31075,\n",
       "    264,\n",
       "    1399,\n",
       "    51360],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1263718605041504,\n",
       "   'compression_ratio': 1.9214285714285715,\n",
       "   'no_speech_prob': 0.0037031839601695538},\n",
       "  {'id': 31,\n",
       "   'seek': 26164,\n",
       "   'start': 261.71999999999997,\n",
       "   'end': 274.12,\n",
       "   'text': ' when new input is given ok. So, let me before go to the example let me show this itself.',\n",
       "   'tokens': [50368,\n",
       "    562,\n",
       "    777,\n",
       "    4846,\n",
       "    307,\n",
       "    2212,\n",
       "    3133,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    718,\n",
       "    385,\n",
       "    949,\n",
       "    352,\n",
       "    281,\n",
       "    264,\n",
       "    1365,\n",
       "    718,\n",
       "    385,\n",
       "    855,\n",
       "    341,\n",
       "    2564,\n",
       "    13,\n",
       "    50988],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.33094546135435715,\n",
       "   'compression_ratio': 1.4108527131782946,\n",
       "   'no_speech_prob': 0.002606326714158058},\n",
       "  {'id': 32,\n",
       "   'seek': 26164,\n",
       "   'start': 276.84,\n",
       "   'end': 286.2,\n",
       "   'text': ' So, if you notice here the key difference this is your standard prompting right where you are',\n",
       "   'tokens': [51124,\n",
       "    407,\n",
       "    11,\n",
       "    498,\n",
       "    291,\n",
       "    3449,\n",
       "    510,\n",
       "    264,\n",
       "    2141,\n",
       "    2649,\n",
       "    341,\n",
       "    307,\n",
       "    428,\n",
       "    3832,\n",
       "    12391,\n",
       "    278,\n",
       "    558,\n",
       "    689,\n",
       "    291,\n",
       "    366,\n",
       "    51592],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.33094546135435715,\n",
       "   'compression_ratio': 1.4108527131782946,\n",
       "   'no_speech_prob': 0.002606326714158058},\n",
       "  {'id': 33,\n",
       "   'seek': 28620,\n",
       "   'start': 286.28,\n",
       "   'end': 293.88,\n",
       "   'text': ' still giving you know one short example here and then asking the question the answer that is',\n",
       "   'tokens': [50368,\n",
       "    920,\n",
       "    2902,\n",
       "    291,\n",
       "    458,\n",
       "    472,\n",
       "    2099,\n",
       "    1365,\n",
       "    510,\n",
       "    293,\n",
       "    550,\n",
       "    3365,\n",
       "    264,\n",
       "    1168,\n",
       "    264,\n",
       "    1867,\n",
       "    300,\n",
       "    307,\n",
       "    50748],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19123512698758033,\n",
       "   'compression_ratio': 1.5647058823529412,\n",
       "   'no_speech_prob': 0.0007202590932138264},\n",
       "  {'id': 34,\n",
       "   'seek': 28620,\n",
       "   'start': 293.88,\n",
       "   'end': 300.92,\n",
       "   'text': ' generated is wrong there. By the way if you give the same thing today it will answer',\n",
       "   'tokens': [50748,\n",
       "    10833,\n",
       "    307,\n",
       "    2085,\n",
       "    456,\n",
       "    13,\n",
       "    3146,\n",
       "    264,\n",
       "    636,\n",
       "    498,\n",
       "    291,\n",
       "    976,\n",
       "    264,\n",
       "    912,\n",
       "    551,\n",
       "    965,\n",
       "    309,\n",
       "    486,\n",
       "    1867,\n",
       "    51100],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19123512698758033,\n",
       "   'compression_ratio': 1.5647058823529412,\n",
       "   'no_speech_prob': 0.0007202590932138264},\n",
       "  {'id': 35,\n",
       "   'seek': 28620,\n",
       "   'start': 300.92,\n",
       "   'end': 306.28,\n",
       "   'text': ' the title ok. So, this is when the paper was published or whichever model that they used',\n",
       "   'tokens': [51100,\n",
       "    264,\n",
       "    4876,\n",
       "    3133,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    341,\n",
       "    307,\n",
       "    562,\n",
       "    264,\n",
       "    3035,\n",
       "    390,\n",
       "    6572,\n",
       "    420,\n",
       "    24123,\n",
       "    2316,\n",
       "    300,\n",
       "    436,\n",
       "    1143,\n",
       "    51368],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19123512698758033,\n",
       "   'compression_ratio': 1.5647058823529412,\n",
       "   'no_speech_prob': 0.0007202590932138264},\n",
       "  {'id': 36,\n",
       "   'seek': 30628,\n",
       "   'start': 307.08,\n",
       "   'end': 317.15999999999997,\n",
       "   'text': ' for this but let us focus on the idea right. So, now this blue piece of information that is added',\n",
       "   'tokens': [50404,\n",
       "    337,\n",
       "    341,\n",
       "    457,\n",
       "    718,\n",
       "    505,\n",
       "    1879,\n",
       "    322,\n",
       "    264,\n",
       "    1558,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    586,\n",
       "    341,\n",
       "    3344,\n",
       "    2522,\n",
       "    295,\n",
       "    1589,\n",
       "    300,\n",
       "    307,\n",
       "    3869,\n",
       "    50908],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19176114123800528,\n",
       "   'compression_ratio': 1.630057803468208,\n",
       "   'no_speech_prob': 0.0009566231747157872},\n",
       "  {'id': 37,\n",
       "   'seek': 30628,\n",
       "   'start': 318.2,\n",
       "   'end': 325.71999999999997,\n",
       "   'text': ' into your example which is the one short example that you are giving for the prompt this',\n",
       "   'tokens': [50960,\n",
       "    666,\n",
       "    428,\n",
       "    1365,\n",
       "    597,\n",
       "    307,\n",
       "    264,\n",
       "    472,\n",
       "    2099,\n",
       "    1365,\n",
       "    300,\n",
       "    291,\n",
       "    366,\n",
       "    2902,\n",
       "    337,\n",
       "    264,\n",
       "    12391,\n",
       "    341,\n",
       "    51336],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19176114123800528,\n",
       "   'compression_ratio': 1.630057803468208,\n",
       "   'no_speech_prob': 0.0009566231747157872},\n",
       "  {'id': 38,\n",
       "   'seek': 30628,\n",
       "   'start': 325.71999999999997,\n",
       "   'end': 333.64,\n",
       "   'text': ' one thing is the prompt right right. So, here if you see Roger started with 5 balls two cans of',\n",
       "   'tokens': [51336,\n",
       "    472,\n",
       "    551,\n",
       "    307,\n",
       "    264,\n",
       "    12391,\n",
       "    558,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    510,\n",
       "    498,\n",
       "    291,\n",
       "    536,\n",
       "    17666,\n",
       "    1409,\n",
       "    365,\n",
       "    1025,\n",
       "    9803,\n",
       "    732,\n",
       "    21835,\n",
       "    295,\n",
       "    51732],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19176114123800528,\n",
       "   'compression_ratio': 1.630057803468208,\n",
       "   'no_speech_prob': 0.0009566231747157872},\n",
       "  {'id': 39,\n",
       "   'seek': 33364,\n",
       "   'start': 333.71999999999997,\n",
       "   'end': 342.28,\n",
       "   'text': ' three tennis balls each these six tennis balls so 5 plus 6 equals 11. So, we are kind of showing the',\n",
       "   'tokens': [50368,\n",
       "    1045,\n",
       "    18118,\n",
       "    9803,\n",
       "    1184,\n",
       "    613,\n",
       "    2309,\n",
       "    18118,\n",
       "    9803,\n",
       "    370,\n",
       "    1025,\n",
       "    1804,\n",
       "    1386,\n",
       "    6915,\n",
       "    2975,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    733,\n",
       "    295,\n",
       "    4099,\n",
       "    264,\n",
       "    50796],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1909616334097726,\n",
       "   'compression_ratio': 1.6333333333333333,\n",
       "   'no_speech_prob': 0.0023165326565504074},\n",
       "  {'id': 40,\n",
       "   'seek': 33364,\n",
       "   'start': 342.28,\n",
       "   'end': 355.64,\n",
       "   'text': ' steps as part of the demonstration itself instead of just trying to ask. So, it is a more detailed',\n",
       "   'tokens': [50796,\n",
       "    4439,\n",
       "    382,\n",
       "    644,\n",
       "    295,\n",
       "    264,\n",
       "    16520,\n",
       "    2564,\n",
       "    2602,\n",
       "    295,\n",
       "    445,\n",
       "    1382,\n",
       "    281,\n",
       "    1029,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    309,\n",
       "    307,\n",
       "    257,\n",
       "    544,\n",
       "    9942,\n",
       "    51464],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1909616334097726,\n",
       "   'compression_ratio': 1.6333333333333333,\n",
       "   'no_speech_prob': 0.0023165326565504074},\n",
       "  {'id': 41,\n",
       "   'seek': 33364,\n",
       "   'start': 355.64,\n",
       "   'end': 362.59999999999997,\n",
       "   'text': ' more specific information that we are illustrating that do this kind of a thing and answer the',\n",
       "   'tokens': [51464,\n",
       "    544,\n",
       "    2685,\n",
       "    1589,\n",
       "    300,\n",
       "    321,\n",
       "    366,\n",
       "    8490,\n",
       "    8754,\n",
       "    300,\n",
       "    360,\n",
       "    341,\n",
       "    733,\n",
       "    295,\n",
       "    257,\n",
       "    551,\n",
       "    293,\n",
       "    1867,\n",
       "    264,\n",
       "    51812],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1909616334097726,\n",
       "   'compression_ratio': 1.6333333333333333,\n",
       "   'no_speech_prob': 0.0023165326565504074},\n",
       "  {'id': 42,\n",
       "   'seek': 36260,\n",
       "   'start': 362.6,\n",
       "   'end': 373.64000000000004,\n",
       "   'text': ' question then just trying to generate the answer ok. So, what LLM then does is given this question',\n",
       "   'tokens': [50364,\n",
       "    1168,\n",
       "    550,\n",
       "    445,\n",
       "    1382,\n",
       "    281,\n",
       "    8460,\n",
       "    264,\n",
       "    1867,\n",
       "    3133,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    437,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    550,\n",
       "    775,\n",
       "    307,\n",
       "    2212,\n",
       "    341,\n",
       "    1168,\n",
       "    50916],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18563575175271105,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0010399200255051255},\n",
       "  {'id': 43,\n",
       "   'seek': 36260,\n",
       "   'start': 373.64000000000004,\n",
       "   'end': 379.72,\n",
       "   'text': ' look at the output now that green piece is the chain of thought it is generated by',\n",
       "   'tokens': [50916,\n",
       "    574,\n",
       "    412,\n",
       "    264,\n",
       "    5598,\n",
       "    586,\n",
       "    300,\n",
       "    3092,\n",
       "    2522,\n",
       "    307,\n",
       "    264,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    309,\n",
       "    307,\n",
       "    10833,\n",
       "    538,\n",
       "    51220],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18563575175271105,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0010399200255051255},\n",
       "  {'id': 44,\n",
       "   'seek': 36260,\n",
       "   'start': 381.64000000000004,\n",
       "   'end': 391.64000000000004,\n",
       "   'text': ' learning from one example that is given here ok. And here since LLM spends kind generating this',\n",
       "   'tokens': [51316,\n",
       "    2539,\n",
       "    490,\n",
       "    472,\n",
       "    1365,\n",
       "    300,\n",
       "    307,\n",
       "    2212,\n",
       "    510,\n",
       "    3133,\n",
       "    13,\n",
       "    400,\n",
       "    510,\n",
       "    1670,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    25620,\n",
       "    733,\n",
       "    17746,\n",
       "    341,\n",
       "    51816],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18563575175271105,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0010399200255051255},\n",
       "  {'id': 45,\n",
       "   'seek': 39164,\n",
       "   'start': 391.64,\n",
       "   'end': 399.88,\n",
       "   'text': ' information ok and that information broken down like which may be implicitly available in the',\n",
       "   'tokens': [50364,\n",
       "    1589,\n",
       "    3133,\n",
       "    293,\n",
       "    300,\n",
       "    1589,\n",
       "    5463,\n",
       "    760,\n",
       "    411,\n",
       "    597,\n",
       "    815,\n",
       "    312,\n",
       "    26947,\n",
       "    356,\n",
       "    2435,\n",
       "    294,\n",
       "    264,\n",
       "    50776],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12619736320094058,\n",
       "   'compression_ratio': 1.5964912280701755,\n",
       "   'no_speech_prob': 0.0013944829115644097},\n",
       "  {'id': 46,\n",
       "   'seek': 39164,\n",
       "   'start': 399.88,\n",
       "   'end': 404.76,\n",
       "   'text': ' problem statement all that gets more explicitly made available as part of a context.',\n",
       "   'tokens': [50776,\n",
       "    1154,\n",
       "    5629,\n",
       "    439,\n",
       "    300,\n",
       "    2170,\n",
       "    544,\n",
       "    20803,\n",
       "    1027,\n",
       "    2435,\n",
       "    382,\n",
       "    644,\n",
       "    295,\n",
       "    257,\n",
       "    4319,\n",
       "    13,\n",
       "    51020],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12619736320094058,\n",
       "   'compression_ratio': 1.5964912280701755,\n",
       "   'no_speech_prob': 0.0013944829115644097},\n",
       "  {'id': 47,\n",
       "   'seek': 39164,\n",
       "   'start': 405.8,\n",
       "   'end': 414.59999999999997,\n",
       "   'text': ' Purely by using verbal reasoning the answer that is generated can be much better and much more',\n",
       "   'tokens': [51072,\n",
       "    29474,\n",
       "    356,\n",
       "    538,\n",
       "    1228,\n",
       "    24781,\n",
       "    21577,\n",
       "    264,\n",
       "    1867,\n",
       "    300,\n",
       "    307,\n",
       "    10833,\n",
       "    393,\n",
       "    312,\n",
       "    709,\n",
       "    1101,\n",
       "    293,\n",
       "    709,\n",
       "    544,\n",
       "    51512],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12619736320094058,\n",
       "   'compression_ratio': 1.5964912280701755,\n",
       "   'no_speech_prob': 0.0013944829115644097},\n",
       "  {'id': 48,\n",
       "   'seek': 41460,\n",
       "   'start': 414.68,\n",
       "   'end': 422.92,\n",
       "   'text': ' accurate than trying to do that from the original original problem and directly trying to guess the answer',\n",
       "   'tokens': [50368,\n",
       "    8559,\n",
       "    813,\n",
       "    1382,\n",
       "    281,\n",
       "    360,\n",
       "    300,\n",
       "    490,\n",
       "    264,\n",
       "    3380,\n",
       "    3380,\n",
       "    1154,\n",
       "    293,\n",
       "    3838,\n",
       "    1382,\n",
       "    281,\n",
       "    2041,\n",
       "    264,\n",
       "    1867,\n",
       "    50780],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.21353167646071491,\n",
       "   'compression_ratio': 1.6775956284153006,\n",
       "   'no_speech_prob': 0.005414759740233421},\n",
       "  {'id': 49,\n",
       "   'seek': 41460,\n",
       "   'start': 424.36,\n",
       "   'end': 431.40000000000003,\n",
       "   'text': ' ok. So, it is almost like do not guess the answer but follow some steps to break down the information',\n",
       "   'tokens': [50852,\n",
       "    3133,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    309,\n",
       "    307,\n",
       "    1920,\n",
       "    411,\n",
       "    360,\n",
       "    406,\n",
       "    2041,\n",
       "    264,\n",
       "    1867,\n",
       "    457,\n",
       "    1524,\n",
       "    512,\n",
       "    4439,\n",
       "    281,\n",
       "    1821,\n",
       "    760,\n",
       "    264,\n",
       "    1589,\n",
       "    51204],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.21353167646071491,\n",
       "   'compression_ratio': 1.6775956284153006,\n",
       "   'no_speech_prob': 0.005414759740233421},\n",
       "  {'id': 50,\n",
       "   'seek': 41460,\n",
       "   'start': 431.88,\n",
       "   'end': 438.28000000000003,\n",
       "   'text': ' and the chain of thought how will you go about doing this and then generate those steps as well as',\n",
       "   'tokens': [51228,\n",
       "    293,\n",
       "    264,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    577,\n",
       "    486,\n",
       "    291,\n",
       "    352,\n",
       "    466,\n",
       "    884,\n",
       "    341,\n",
       "    293,\n",
       "    550,\n",
       "    8460,\n",
       "    729,\n",
       "    4439,\n",
       "    382,\n",
       "    731,\n",
       "    382,\n",
       "    51548],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.21353167646071491,\n",
       "   'compression_ratio': 1.6775956284153006,\n",
       "   'no_speech_prob': 0.005414759740233421},\n",
       "  {'id': 51,\n",
       "   'seek': 43828,\n",
       "   'start': 438.76,\n",
       "   'end': 446.76,\n",
       "   'text': ' the answer ok. This helps the LLM to focus on the right kind of information and the right',\n",
       "   'tokens': [50388,\n",
       "    264,\n",
       "    1867,\n",
       "    3133,\n",
       "    13,\n",
       "    639,\n",
       "    3665,\n",
       "    264,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    281,\n",
       "    1879,\n",
       "    322,\n",
       "    264,\n",
       "    558,\n",
       "    733,\n",
       "    295,\n",
       "    1589,\n",
       "    293,\n",
       "    264,\n",
       "    558,\n",
       "    50788],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20447365182345031,\n",
       "   'compression_ratio': 1.6139240506329113,\n",
       "   'no_speech_prob': 0.00011597533739404753},\n",
       "  {'id': 52,\n",
       "   'seek': 43828,\n",
       "   'start': 452.76,\n",
       "   'end': 458.44,\n",
       "   'text': ' in the problem and then generate the answer. So, the context that it gets is much much better',\n",
       "   'tokens': [51088,\n",
       "    294,\n",
       "    264,\n",
       "    1154,\n",
       "    293,\n",
       "    550,\n",
       "    8460,\n",
       "    264,\n",
       "    1867,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    264,\n",
       "    4319,\n",
       "    300,\n",
       "    309,\n",
       "    2170,\n",
       "    307,\n",
       "    709,\n",
       "    709,\n",
       "    1101,\n",
       "    51372],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20447365182345031,\n",
       "   'compression_ratio': 1.6139240506329113,\n",
       "   'no_speech_prob': 0.00011597533739404753},\n",
       "  {'id': 53,\n",
       "   'seek': 43828,\n",
       "   'start': 460.35999999999996,\n",
       "   'end': 464.2,\n",
       "   'text': ' for answering the question than the original question is this is clear.',\n",
       "   'tokens': [51468,\n",
       "    337,\n",
       "    13430,\n",
       "    264,\n",
       "    1168,\n",
       "    813,\n",
       "    264,\n",
       "    3380,\n",
       "    1168,\n",
       "    307,\n",
       "    341,\n",
       "    307,\n",
       "    1850,\n",
       "    13,\n",
       "    51660],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20447365182345031,\n",
       "   'compression_ratio': 1.6139240506329113,\n",
       "   'no_speech_prob': 0.00011597533739404753},\n",
       "  {'id': 54,\n",
       "   'seek': 46828,\n",
       "   'start': 468.52,\n",
       "   'end': 476.52,\n",
       "   'text': ' So, not like the the detailed prompts I mean it is not like they are just using one example',\n",
       "   'tokens': [50376,\n",
       "    407,\n",
       "    11,\n",
       "    406,\n",
       "    411,\n",
       "    264,\n",
       "    264,\n",
       "    9942,\n",
       "    41095,\n",
       "    286,\n",
       "    914,\n",
       "    309,\n",
       "    307,\n",
       "    406,\n",
       "    411,\n",
       "    436,\n",
       "    366,\n",
       "    445,\n",
       "    1228,\n",
       "    472,\n",
       "    1365,\n",
       "    50776],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13362967266755946,\n",
       "   'compression_ratio': 1.7169811320754718,\n",
       "   'no_speech_prob': 0.001080236630514264},\n",
       "  {'id': 55,\n",
       "   'seek': 46828,\n",
       "   'start': 476.52,\n",
       "   'end': 483.55999999999995,\n",
       "   'text': ' like I think more examples are used as part of the prompting for the sake of brevity in',\n",
       "   'tokens': [50776,\n",
       "    411,\n",
       "    286,\n",
       "    519,\n",
       "    544,\n",
       "    5110,\n",
       "    366,\n",
       "    1143,\n",
       "    382,\n",
       "    644,\n",
       "    295,\n",
       "    264,\n",
       "    12391,\n",
       "    278,\n",
       "    337,\n",
       "    264,\n",
       "    9717,\n",
       "    295,\n",
       "    1403,\n",
       "    23110,\n",
       "    294,\n",
       "    51128],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13362967266755946,\n",
       "   'compression_ratio': 1.7169811320754718,\n",
       "   'no_speech_prob': 0.001080236630514264},\n",
       "  {'id': 56,\n",
       "   'seek': 46828,\n",
       "   'start': 483.55999999999995,\n",
       "   'end': 488.35999999999996,\n",
       "   'text': ' presentation they are just showing one example. So, if you look at the paper you can see more',\n",
       "   'tokens': [51128,\n",
       "    5860,\n",
       "    436,\n",
       "    366,\n",
       "    445,\n",
       "    4099,\n",
       "    472,\n",
       "    1365,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    498,\n",
       "    291,\n",
       "    574,\n",
       "    412,\n",
       "    264,\n",
       "    3035,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    544,\n",
       "    51368],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13362967266755946,\n",
       "   'compression_ratio': 1.7169811320754718,\n",
       "   'no_speech_prob': 0.001080236630514264},\n",
       "  {'id': 57,\n",
       "   'seek': 48836,\n",
       "   'start': 488.44,\n",
       "   'end': 497.32,\n",
       "   'text': ' detailed prompts in the end. But the idea remains just the same right. So, we are',\n",
       "   'tokens': [50368,\n",
       "    9942,\n",
       "    41095,\n",
       "    294,\n",
       "    264,\n",
       "    917,\n",
       "    13,\n",
       "    583,\n",
       "    264,\n",
       "    1558,\n",
       "    7023,\n",
       "    445,\n",
       "    264,\n",
       "    912,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    50812],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19272933900356293,\n",
       "   'compression_ratio': 1.6538461538461537,\n",
       "   'no_speech_prob': 0.0018331888131797314},\n",
       "  {'id': 58,\n",
       "   'seek': 48836,\n",
       "   'start': 497.32,\n",
       "   'end': 503.0,\n",
       "   'text': ' inducing a particular style of thinking by giving examples of that style of thinking.',\n",
       "   'tokens': [50812,\n",
       "    13716,\n",
       "    2175,\n",
       "    257,\n",
       "    1729,\n",
       "    3758,\n",
       "    295,\n",
       "    1953,\n",
       "    538,\n",
       "    2902,\n",
       "    5110,\n",
       "    295,\n",
       "    300,\n",
       "    3758,\n",
       "    295,\n",
       "    1953,\n",
       "    13,\n",
       "    51096],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19272933900356293,\n",
       "   'compression_ratio': 1.6538461538461537,\n",
       "   'no_speech_prob': 0.0018331888131797314},\n",
       "  {'id': 59,\n",
       "   'seek': 48836,\n",
       "   'start': 505.64,\n",
       "   'end': 513.4,\n",
       "   'text': ' So, as such it is just a few short example but the way we are structuring the examples and',\n",
       "   'tokens': [51228,\n",
       "    407,\n",
       "    11,\n",
       "    382,\n",
       "    1270,\n",
       "    309,\n",
       "    307,\n",
       "    445,\n",
       "    257,\n",
       "    1326,\n",
       "    2099,\n",
       "    1365,\n",
       "    457,\n",
       "    264,\n",
       "    636,\n",
       "    321,\n",
       "    366,\n",
       "    6594,\n",
       "    1345,\n",
       "    264,\n",
       "    5110,\n",
       "    293,\n",
       "    51616],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19272933900356293,\n",
       "   'compression_ratio': 1.6538461538461537,\n",
       "   'no_speech_prob': 0.0018331888131797314},\n",
       "  {'id': 60,\n",
       "   'seek': 51340,\n",
       "   'start': 513.48,\n",
       "   'end': 521.9599999999999,\n",
       "   'text': ' demonstrations is where we have control. It is almost like we are using a solved problems kind',\n",
       "   'tokens': [50368,\n",
       "    34714,\n",
       "    307,\n",
       "    689,\n",
       "    321,\n",
       "    362,\n",
       "    1969,\n",
       "    13,\n",
       "    467,\n",
       "    307,\n",
       "    1920,\n",
       "    411,\n",
       "    321,\n",
       "    366,\n",
       "    1228,\n",
       "    257,\n",
       "    13041,\n",
       "    2740,\n",
       "    733,\n",
       "    50792],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18184697823446305,\n",
       "   'compression_ratio': 1.6107784431137724,\n",
       "   'no_speech_prob': 0.0007549145957455039},\n",
       "  {'id': 61,\n",
       "   'seek': 51340,\n",
       "   'start': 521.9599999999999,\n",
       "   'end': 528.04,\n",
       "   'text': ' of the context we are giving to the model and kind of encouraging it to use similar kind of',\n",
       "   'tokens': [50792,\n",
       "    295,\n",
       "    264,\n",
       "    4319,\n",
       "    321,\n",
       "    366,\n",
       "    2902,\n",
       "    281,\n",
       "    264,\n",
       "    2316,\n",
       "    293,\n",
       "    733,\n",
       "    295,\n",
       "    14580,\n",
       "    309,\n",
       "    281,\n",
       "    764,\n",
       "    2531,\n",
       "    733,\n",
       "    295,\n",
       "    51096],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18184697823446305,\n",
       "   'compression_ratio': 1.6107784431137724,\n",
       "   'no_speech_prob': 0.0007549145957455039},\n",
       "  {'id': 62,\n",
       "   'seek': 51340,\n",
       "   'start': 528.04,\n",
       "   'end': 543.0799999999999,\n",
       "   'text': ' thinking to solve any new problem given. So, there are you know more examples here',\n",
       "   'tokens': [51096,\n",
       "    1953,\n",
       "    281,\n",
       "    5039,\n",
       "    604,\n",
       "    777,\n",
       "    1154,\n",
       "    2212,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    456,\n",
       "    366,\n",
       "    291,\n",
       "    458,\n",
       "    544,\n",
       "    5110,\n",
       "    510,\n",
       "    51848],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18184697823446305,\n",
       "   'compression_ratio': 1.6107784431137724,\n",
       "   'no_speech_prob': 0.0007549145957455039},\n",
       "  {'id': 63,\n",
       "   'seek': 54340,\n",
       "   'start': 543.88,\n",
       "   'end': 552.1999999999999,\n",
       "   'text': ' how the model you kind of breaks down for different types of problems how the',\n",
       "   'tokens': [50388,\n",
       "    577,\n",
       "    264,\n",
       "    2316,\n",
       "    291,\n",
       "    733,\n",
       "    295,\n",
       "    9857,\n",
       "    760,\n",
       "    337,\n",
       "    819,\n",
       "    3467,\n",
       "    295,\n",
       "    2740,\n",
       "    577,\n",
       "    264,\n",
       "    50804],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2194676477400983,\n",
       "   'compression_ratio': 1.622754491017964,\n",
       "   'no_speech_prob': 0.0021410484332591295},\n",
       "  {'id': 64,\n",
       "   'seek': 54340,\n",
       "   'start': 552.1999999999999,\n",
       "   'end': 558.36,\n",
       "   'text': ' information the question is broken down into your chain of thought and the answer is provided.',\n",
       "   'tokens': [50804,\n",
       "    1589,\n",
       "    264,\n",
       "    1168,\n",
       "    307,\n",
       "    5463,\n",
       "    760,\n",
       "    666,\n",
       "    428,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    293,\n",
       "    264,\n",
       "    1867,\n",
       "    307,\n",
       "    5649,\n",
       "    13,\n",
       "    51112],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2194676477400983,\n",
       "   'compression_ratio': 1.622754491017964,\n",
       "   'no_speech_prob': 0.0021410484332591295},\n",
       "  {'id': 65,\n",
       "   'seek': 54340,\n",
       "   'start': 558.36,\n",
       "   'end': 566.4399999999999,\n",
       "   'text': ' So, you might be finally interested only in the answer right. So, you have the option of inserting',\n",
       "   'tokens': [51112,\n",
       "    407,\n",
       "    11,\n",
       "    291,\n",
       "    1062,\n",
       "    312,\n",
       "    2721,\n",
       "    3102,\n",
       "    787,\n",
       "    294,\n",
       "    264,\n",
       "    1867,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    291,\n",
       "    362,\n",
       "    264,\n",
       "    3614,\n",
       "    295,\n",
       "    46567,\n",
       "    51516],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2194676477400983,\n",
       "   'compression_ratio': 1.622754491017964,\n",
       "   'no_speech_prob': 0.0021410484332591295},\n",
       "  {'id': 66,\n",
       "   'seek': 56644,\n",
       "   'start': 566.9200000000001,\n",
       "   'end': 574.44,\n",
       "   'text': ' okay you do not have to follow the exact same pattern right. You could you know model this thing',\n",
       "   'tokens': [50388,\n",
       "    1392,\n",
       "    291,\n",
       "    360,\n",
       "    406,\n",
       "    362,\n",
       "    281,\n",
       "    1524,\n",
       "    264,\n",
       "    1900,\n",
       "    912,\n",
       "    5102,\n",
       "    558,\n",
       "    13,\n",
       "    509,\n",
       "    727,\n",
       "    291,\n",
       "    458,\n",
       "    2316,\n",
       "    341,\n",
       "    551,\n",
       "    50764],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.4250868507053541,\n",
       "   'compression_ratio': 1.3873239436619718,\n",
       "   'no_speech_prob': 0.011704937554895878},\n",
       "  {'id': 67,\n",
       "   'seek': 56644,\n",
       "   'start': 574.9200000000001,\n",
       "   'end': 586.6800000000001,\n",
       "   'text': ' one second object like create one place where okay. So, you can say for any your demonstrations with',\n",
       "   'tokens': [50788,\n",
       "    472,\n",
       "    1150,\n",
       "    2657,\n",
       "    411,\n",
       "    1884,\n",
       "    472,\n",
       "    1081,\n",
       "    689,\n",
       "    1392,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    584,\n",
       "    337,\n",
       "    604,\n",
       "    428,\n",
       "    34714,\n",
       "    365,\n",
       "    51376],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.4250868507053541,\n",
       "   'compression_ratio': 1.3873239436619718,\n",
       "   'no_speech_prob': 0.011704937554895878},\n",
       "  {'id': 68,\n",
       "   'seek': 58668,\n",
       "   'start': 586.68,\n",
       "   'end': 615.8,\n",
       "   'text': ' okay. So, you go that part. So, we will have like you can say question',\n",
       "   'tokens': [50364,\n",
       "    1392,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    291,\n",
       "    352,\n",
       "    300,\n",
       "    644,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    321,\n",
       "    486,\n",
       "    362,\n",
       "    411,\n",
       "    291,\n",
       "    393,\n",
       "    584,\n",
       "    1168,\n",
       "    51820],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.5521087231843368,\n",
       "   'compression_ratio': 1.0144927536231885,\n",
       "   'no_speech_prob': 0.07439953833818436},\n",
       "  {'id': 69,\n",
       "   'seek': 61580,\n",
       "   'start': 616.52,\n",
       "   'end': 630.28,\n",
       "   'text': ' no this is back okay. You can use this question and give something you can say reasoning',\n",
       "   'tokens': [50400,\n",
       "    572,\n",
       "    341,\n",
       "    307,\n",
       "    646,\n",
       "    1392,\n",
       "    13,\n",
       "    509,\n",
       "    393,\n",
       "    764,\n",
       "    341,\n",
       "    1168,\n",
       "    293,\n",
       "    976,\n",
       "    746,\n",
       "    291,\n",
       "    393,\n",
       "    584,\n",
       "    21577,\n",
       "    51088],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.24313585148301237,\n",
       "   'compression_ratio': 1.5689655172413792,\n",
       "   'no_speech_prob': 0.008492713794112206},\n",
       "  {'id': 70,\n",
       "   'seek': 61580,\n",
       "   'start': 634.12,\n",
       "   'end': 640.1999999999999,\n",
       "   'text': ' and then you can provide something and then you can say answer is this right. You could break',\n",
       "   'tokens': [51280,\n",
       "    293,\n",
       "    550,\n",
       "    291,\n",
       "    393,\n",
       "    2893,\n",
       "    746,\n",
       "    293,\n",
       "    550,\n",
       "    291,\n",
       "    393,\n",
       "    584,\n",
       "    1867,\n",
       "    307,\n",
       "    341,\n",
       "    558,\n",
       "    13,\n",
       "    509,\n",
       "    727,\n",
       "    1821,\n",
       "    51584],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.24313585148301237,\n",
       "   'compression_ratio': 1.5689655172413792,\n",
       "   'no_speech_prob': 0.008492713794112206},\n",
       "  {'id': 71,\n",
       "   'seek': 64020,\n",
       "   'start': 640.2800000000001,\n",
       "   'end': 647.32,\n",
       "   'text': ' that down explicitly also which is what will be done by later to the props. But the idea is still',\n",
       "   'tokens': [50368,\n",
       "    300,\n",
       "    760,\n",
       "    20803,\n",
       "    611,\n",
       "    597,\n",
       "    307,\n",
       "    437,\n",
       "    486,\n",
       "    312,\n",
       "    1096,\n",
       "    538,\n",
       "    1780,\n",
       "    281,\n",
       "    264,\n",
       "    26173,\n",
       "    13,\n",
       "    583,\n",
       "    264,\n",
       "    1558,\n",
       "    307,\n",
       "    920,\n",
       "    50720],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18190765380859375,\n",
       "   'compression_ratio': 1.819047619047619,\n",
       "   'no_speech_prob': 0.0053641037084162235},\n",
       "  {'id': 72,\n",
       "   'seek': 64020,\n",
       "   'start': 647.32,\n",
       "   'end': 653.96,\n",
       "   'text': ' the same right. So, but this will help you separate the reasoning and answer maybe for your',\n",
       "   'tokens': [50720,\n",
       "    264,\n",
       "    912,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    457,\n",
       "    341,\n",
       "    486,\n",
       "    854,\n",
       "    291,\n",
       "    4994,\n",
       "    264,\n",
       "    21577,\n",
       "    293,\n",
       "    1867,\n",
       "    1310,\n",
       "    337,\n",
       "    428,\n",
       "    51052],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18190765380859375,\n",
       "   'compression_ratio': 1.819047619047619,\n",
       "   'no_speech_prob': 0.0053641037084162235},\n",
       "  {'id': 73,\n",
       "   'seek': 64020,\n",
       "   'start': 653.96,\n",
       "   'end': 660.5200000000001,\n",
       "   'text': ' parsing stages later or you want to take the reasoning and maybe you want to use that reasoning',\n",
       "   'tokens': [51052,\n",
       "    21156,\n",
       "    278,\n",
       "    10232,\n",
       "    1780,\n",
       "    420,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    747,\n",
       "    264,\n",
       "    21577,\n",
       "    293,\n",
       "    1310,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    764,\n",
       "    300,\n",
       "    21577,\n",
       "    51380],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18190765380859375,\n",
       "   'compression_ratio': 1.819047619047619,\n",
       "   'no_speech_prob': 0.0053641037084162235},\n",
       "  {'id': 74,\n",
       "   'seek': 64020,\n",
       "   'start': 660.5200000000001,\n",
       "   'end': 666.5200000000001,\n",
       "   'text': ' and use an external tool to solve the problem and fill it in here. So, this kind of this kind of',\n",
       "   'tokens': [51380,\n",
       "    293,\n",
       "    764,\n",
       "    364,\n",
       "    8320,\n",
       "    2290,\n",
       "    281,\n",
       "    5039,\n",
       "    264,\n",
       "    1154,\n",
       "    293,\n",
       "    2836,\n",
       "    309,\n",
       "    294,\n",
       "    510,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    341,\n",
       "    733,\n",
       "    295,\n",
       "    341,\n",
       "    733,\n",
       "    295,\n",
       "    51680],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18190765380859375,\n",
       "   'compression_ratio': 1.819047619047619,\n",
       "   'no_speech_prob': 0.0053641037084162235},\n",
       "  {'id': 75,\n",
       "   'seek': 66652,\n",
       "   'start': 666.6,\n",
       "   'end': 675.3199999999999,\n",
       "   'text': \" adding these additional scaffolding's can give you little more control. But the idea is\",\n",
       "   'tokens': [50368,\n",
       "    5127,\n",
       "    613,\n",
       "    4497,\n",
       "    44094,\n",
       "    278,\n",
       "    311,\n",
       "    393,\n",
       "    976,\n",
       "    291,\n",
       "    707,\n",
       "    544,\n",
       "    1969,\n",
       "    13,\n",
       "    583,\n",
       "    264,\n",
       "    1558,\n",
       "    307,\n",
       "    50804],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1790618747472763,\n",
       "   'compression_ratio': 1.4893617021276595,\n",
       "   'no_speech_prob': 0.0018210930284112692},\n",
       "  {'id': 76,\n",
       "   'seek': 66652,\n",
       "   'start': 676.52,\n",
       "   'end': 683.48,\n",
       "   'text': ' the way we want the model to think we can induce that thinking by just giving examples through',\n",
       "   'tokens': [50864,\n",
       "    264,\n",
       "    636,\n",
       "    321,\n",
       "    528,\n",
       "    264,\n",
       "    2316,\n",
       "    281,\n",
       "    519,\n",
       "    321,\n",
       "    393,\n",
       "    41263,\n",
       "    300,\n",
       "    1953,\n",
       "    538,\n",
       "    445,\n",
       "    2902,\n",
       "    5110,\n",
       "    807,\n",
       "    51212],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1790618747472763,\n",
       "   'compression_ratio': 1.4893617021276595,\n",
       "   'no_speech_prob': 0.0018210930284112692},\n",
       "  {'id': 77,\n",
       "   'seek': 66652,\n",
       "   'start': 683.48,\n",
       "   'end': 695.3199999999999,\n",
       "   'text': ' few short prompting right. So, what are some important advantages here? You can help us decompose',\n",
       "   'tokens': [51212,\n",
       "    1326,\n",
       "    2099,\n",
       "    12391,\n",
       "    278,\n",
       "    558,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    437,\n",
       "    366,\n",
       "    512,\n",
       "    1021,\n",
       "    14906,\n",
       "    510,\n",
       "    30,\n",
       "    509,\n",
       "    393,\n",
       "    854,\n",
       "    505,\n",
       "    22867,\n",
       "    541,\n",
       "    51804],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1790618747472763,\n",
       "   'compression_ratio': 1.4893617021276595,\n",
       "   'no_speech_prob': 0.0018210930284112692},\n",
       "  {'id': 78,\n",
       "   'seek': 69532,\n",
       "   'start': 695.96,\n",
       "   'end': 701.1600000000001,\n",
       "   'text': ' the problem complex problem into intermediate steps allowing more information processing',\n",
       "   'tokens': [50396,\n",
       "    264,\n",
       "    1154,\n",
       "    3997,\n",
       "    1154,\n",
       "    666,\n",
       "    19376,\n",
       "    4439,\n",
       "    8293,\n",
       "    544,\n",
       "    1589,\n",
       "    9007,\n",
       "    50656],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14365714109396632,\n",
       "   'compression_ratio': 1.6712328767123288,\n",
       "   'no_speech_prob': 0.00036772008752450347},\n",
       "  {'id': 79,\n",
       "   'seek': 69532,\n",
       "   'start': 702.5200000000001,\n",
       "   'end': 711.4000000000001,\n",
       "   'text': ' you know for the LLM to generate the reliable responses. So, interpretability if you want to use',\n",
       "   'tokens': [50724,\n",
       "    291,\n",
       "    458,\n",
       "    337,\n",
       "    264,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    281,\n",
       "    8460,\n",
       "    264,\n",
       "    12924,\n",
       "    13019,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    7302,\n",
       "    2310,\n",
       "    498,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    764,\n",
       "    51168],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14365714109396632,\n",
       "   'compression_ratio': 1.6712328767123288,\n",
       "   'no_speech_prob': 0.00036772008752450347},\n",
       "  {'id': 80,\n",
       "   'seek': 69532,\n",
       "   'start': 711.4000000000001,\n",
       "   'end': 718.0400000000001,\n",
       "   'text': ' the reasoning that is generated to give it to the end user or use it in some other way in',\n",
       "   'tokens': [51168,\n",
       "    264,\n",
       "    21577,\n",
       "    300,\n",
       "    307,\n",
       "    10833,\n",
       "    281,\n",
       "    976,\n",
       "    309,\n",
       "    281,\n",
       "    264,\n",
       "    917,\n",
       "    4195,\n",
       "    420,\n",
       "    764,\n",
       "    309,\n",
       "    294,\n",
       "    512,\n",
       "    661,\n",
       "    636,\n",
       "    294,\n",
       "    51500],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14365714109396632,\n",
       "   'compression_ratio': 1.6712328767123288,\n",
       "   'no_speech_prob': 0.00036772008752450347},\n",
       "  {'id': 81,\n",
       "   'seek': 69532,\n",
       "   'start': 718.0400000000001,\n",
       "   'end': 723.48,\n",
       "   'text': ' order to understand the model behavior on why this answer was suggested you could use this',\n",
       "   'tokens': [51500,\n",
       "    1668,\n",
       "    281,\n",
       "    1223,\n",
       "    264,\n",
       "    2316,\n",
       "    5223,\n",
       "    322,\n",
       "    983,\n",
       "    341,\n",
       "    1867,\n",
       "    390,\n",
       "    10945,\n",
       "    291,\n",
       "    727,\n",
       "    764,\n",
       "    341,\n",
       "    51772],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14365714109396632,\n",
       "   'compression_ratio': 1.6712328767123288,\n",
       "   'no_speech_prob': 0.00036772008752450347},\n",
       "  {'id': 82,\n",
       "   'seek': 72348,\n",
       "   'start': 723.48,\n",
       "   'end': 734.44,\n",
       "   'text': ' end user. So, useful for you know math problems or reasoning where multi-step reasoning is involved',\n",
       "   'tokens': [50364,\n",
       "    917,\n",
       "    4195,\n",
       "    13,\n",
       "    407,\n",
       "    11,\n",
       "    4420,\n",
       "    337,\n",
       "    291,\n",
       "    458,\n",
       "    5221,\n",
       "    2740,\n",
       "    420,\n",
       "    21577,\n",
       "    689,\n",
       "    4825,\n",
       "    12,\n",
       "    16792,\n",
       "    21577,\n",
       "    307,\n",
       "    3288,\n",
       "    50912],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2604854071318214,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0007643308490514755},\n",
       "  {'id': 83,\n",
       "   'seek': 72348,\n",
       "   'start': 735.96,\n",
       "   'end': 742.28,\n",
       "   'text': ' so, it is not just I can I cannot read I cannot read the answer from the question directly.',\n",
       "   'tokens': [50988,\n",
       "    370,\n",
       "    11,\n",
       "    309,\n",
       "    307,\n",
       "    406,\n",
       "    445,\n",
       "    286,\n",
       "    393,\n",
       "    286,\n",
       "    2644,\n",
       "    1401,\n",
       "    286,\n",
       "    2644,\n",
       "    1401,\n",
       "    264,\n",
       "    1867,\n",
       "    490,\n",
       "    264,\n",
       "    1168,\n",
       "    3838,\n",
       "    13,\n",
       "    51304],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2604854071318214,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0007643308490514755},\n",
       "  {'id': 84,\n",
       "   'seek': 72348,\n",
       "   'start': 742.28,\n",
       "   'end': 747.8000000000001,\n",
       "   'text': ' I need to do few steps before answering there you can use chain of thought prompting.',\n",
       "   'tokens': [51304,\n",
       "    286,\n",
       "    643,\n",
       "    281,\n",
       "    360,\n",
       "    1326,\n",
       "    4439,\n",
       "    949,\n",
       "    13430,\n",
       "    456,\n",
       "    291,\n",
       "    393,\n",
       "    764,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    12391,\n",
       "    278,\n",
       "    13,\n",
       "    51580],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2604854071318214,\n",
       "   'compression_ratio': 1.5649717514124293,\n",
       "   'no_speech_prob': 0.0007643308490514755},\n",
       "  {'id': 85,\n",
       "   'seek': 74780,\n",
       "   'start': 748.3599999999999,\n",
       "   'end': 755.88,\n",
       "   'text': \" And it is easy to elicit in LLM's this kind of a behavior using few short point.\",\n",
       "   'tokens': [50392,\n",
       "    400,\n",
       "    309,\n",
       "    307,\n",
       "    1858,\n",
       "    281,\n",
       "    806,\n",
       "    8876,\n",
       "    294,\n",
       "    441,\n",
       "    43,\n",
       "    44,\n",
       "    311,\n",
       "    341,\n",
       "    733,\n",
       "    295,\n",
       "    257,\n",
       "    5223,\n",
       "    1228,\n",
       "    1326,\n",
       "    2099,\n",
       "    935,\n",
       "    13,\n",
       "    50768],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2772466242313385,\n",
       "   'compression_ratio': 1.4444444444444444,\n",
       "   'no_speech_prob': 0.0023863394744694233},\n",
       "  {'id': 86,\n",
       "   'seek': 74780,\n",
       "   'start': 758.68,\n",
       "   'end': 764.8399999999999,\n",
       "   'text': ' So, any questions about chain of thought because this is going to be a very crucial',\n",
       "   'tokens': [50908,\n",
       "    407,\n",
       "    11,\n",
       "    604,\n",
       "    1651,\n",
       "    466,\n",
       "    5021,\n",
       "    295,\n",
       "    1194,\n",
       "    570,\n",
       "    341,\n",
       "    307,\n",
       "    516,\n",
       "    281,\n",
       "    312,\n",
       "    257,\n",
       "    588,\n",
       "    11462,\n",
       "    51216],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2772466242313385,\n",
       "   'compression_ratio': 1.4444444444444444,\n",
       "   'no_speech_prob': 0.0023863394744694233},\n",
       "  {'id': 87,\n",
       "   'seek': 74780,\n",
       "   'start': 765.56,\n",
       "   'end': 775.64,\n",
       "   'text': ' technique that you will have to keep in mind many of them will build on the focus.',\n",
       "   'tokens': [51252,\n",
       "    6532,\n",
       "    300,\n",
       "    291,\n",
       "    486,\n",
       "    362,\n",
       "    281,\n",
       "    1066,\n",
       "    294,\n",
       "    1575,\n",
       "    867,\n",
       "    295,\n",
       "    552,\n",
       "    486,\n",
       "    1322,\n",
       "    322,\n",
       "    264,\n",
       "    1879,\n",
       "    13,\n",
       "    51756],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2772466242313385,\n",
       "   'compression_ratio': 1.4444444444444444,\n",
       "   'no_speech_prob': 0.0023863394744694233}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"For a given transcription from a video to be published online, give a very short pre-read for the viewers on what to expect from the video\"),\n",
    "    HumanMessage(content=f\"TRANSCRIPTION: {transcription}\\nPREREAD:\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this video, we'll explore the concept of chain of thought prompting in language models, a technique that enhances their ability to perform multi-step reasoning tasks. You'll learn why traditional few-shot prompting might fall short in complex problem-solving scenarios, and how structured examples can guide models to generate more accurate answers by breaking down the reasoning process. Join us as we delve into the nuances of prompting strategies and their implications for improving language model performance!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"For a given transcription from the video published online, give a summary of the video for the viewers\"),\n",
    "    HumanMessage(content=f\"TRANSCRIPTION: {transcription}\\nSUMMARY:\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this video, the speaker discusses the concept of \"chain of thought prompting\" in large language models (LLMs). They explain that while few-shot prompting can be effective for certain tasks, it struggles with multi-step reasoning problems. The key idea is to encourage LLMs to generate intermediate thoughts rather than jumping directly to an answer. By providing detailed examples that illustrate the reasoning process, LLMs can learn to mimic this thinking style, leading to more accurate responses. The speaker emphasizes that the structure of the examples is crucial in guiding the model\\'s thought process. They highlight the advantages of this method for solving complex problems, improving interpretability, and fostering reliable answers, especially in situations requiring intricate reasoning. The discussion underlines the significance of chain of thought prompting as a foundational technique in the realm of LLMs.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     SystemMessage(content=\"You are an expert teacher in AI. For a given transcription from a video, generate 10 multiple choice questions that cover the topics discussed in this session. The questions are to be of recall-from-content type\"),\n",
    "#     HumanMessage(content=f\"TRANSCRIPTION: {transcription}\\QUESTIONS:\"),\n",
    "# ]\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert teacher in AI. For a given transcription from a video, generate 10 multiple choice questions that cover the topics discussed in this session. The questions are to be of recall-from-content type\"),\n",
    "    HumanMessage(content=f\"TRANSCRIPTION: {transcription}\\QUESTIONS:\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the main problem with few-shot prompting when it comes to multi-step reasoning?\n",
      "   - A) It generates too many answers\n",
      "   - B) It does not effectively guide the model through the necessary reasoning steps\n",
      "   - C) It requires too much data\n",
      "   - D) It is not applicable to language models\n",
      "   \n",
      "2. What is the key idea behind the chain of thought prompting method?\n",
      "   - A) To provide straightforward answers without reasoning\n",
      "   - B) To induce logical, step-by-step thinking in the model\n",
      "   - C) To simplify the input questions\n",
      "   - D) To minimize the number of examples used in prompting\n",
      "\n",
      "3. How does a language model like LLM generate tokens?\n",
      "   - A) By implementing logical reasoning\n",
      "   - B) Based on a series of previous tokens and their probabilities\n",
      "   - C) Through external computational tools\n",
      "   - D) By guessing the answers\n",
      "\n",
      "4. What does the implementation of chain of thought prompting aim to achieve?\n",
      "   - A) Generate random outputs\n",
      "   - B) Enable language models to generate a series of logical steps before arriving at an answer\n",
      "   - C) Eliminate the need for examples\n",
      "   - D) Increase the complexity of the model's architecture\n",
      "\n",
      "5. According to the transcription, what is a significant advantage of using chain of thought prompting?\n",
      "   - A) It reduces computation time\n",
      "   - B) It allows complex problems to be broken down into intermediate steps\n",
      "   - C) It avoids the use of examples altogether\n",
      "   - D) It results in faster training of language models\n",
      "\n",
      "6. In the example provided in the transcription, what was the result of using a standard prompting technique?\n",
      "   - A) The answer generated was correct\n",
      "   - B) The answer generated was wrong\n",
      "   - C) No answer was generated\n",
      "   - D) The model did not understand the question\n",
      "\n",
      "7. How can detailed prompts improve the performance of language models?\n",
      "   - A) By providing straightforward answers\n",
      "   - B) By encouraging the model to follow specific reasoning patterns\n",
      "   - C) By simplifying the input data\n",
      "   - D) By reducing the number of examples needed\n",
      "\n",
      "8. Which type of problems is chain of thought prompting particularly useful for?\n",
      "   - A) Simple factual queries\n",
      "   - B) Math problems or reasoning tasks that require multiple steps\n",
      "   - C) Creative writing tasks\n",
      "   - D) Image recognition tasks\n",
      "\n",
      "9. What type of information does the model focus on when using chain of thought prompting?\n",
      "   - A) The final answer alone\n",
      "   - B) The reasoning process and intermediate steps involved in reaching the answer\n",
      "   - C) The length of the input question\n",
      "   - D) The number of examples provided\n",
      "\n",
      "10. What does the presenter suggest about the model's ability to learn from examples?\n",
      "    - A) It cannot learn from examples at all\n",
      "    - B) It can mimic the process of thinking from given examples\n",
      "    - C) It only learns from incorrect examples\n",
      "    - D) It relies solely on its initial training data without learning from new examples\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manual-script-videos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
